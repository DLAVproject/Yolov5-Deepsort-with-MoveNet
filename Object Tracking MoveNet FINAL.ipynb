{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73c39212-0a76-426d-809a-c6a60cefdb9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Benjamin\\anaconda3\\envs\\yolov4-cpu\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# comment out below line to enable tensorflow logging outputs\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import time\n",
    "import tensorflow as tf\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "if len(physical_devices) > 0:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "from absl import app, flags, logging\n",
    "from absl.flags import FLAGS\n",
    "import core.utils as util\n",
    "from core.yolov4 import filter_boxes\n",
    "from tensorflow.python.saved_model import tag_constants\n",
    "from core.config import cfg\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "# deep sort imports\n",
    "from deep_sort import preprocessing, nn_matching\n",
    "from deep_sort.detection import Detection\n",
    "from deep_sort.tracker import Tracker\n",
    "from tools import generate_detections as gdet\n",
    "\n",
    "\n",
    "import torch\n",
    "import math\n",
    "\n",
    "\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6ed92f4-4417-46c8-b16d-f40a5b2182cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Benjamin/.cache\\torch\\hub\\ultralytics_yolov5_master\n",
      "YOLOv5  2022-4-27 torch 1.11.0 CPU\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s_v6 summary: 213 layers, 7225885 parameters, 0 gradients\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "#load yolov5 for object detection\n",
    "model = torch.hub.load('ultralytics/yolov5', 'yolov5s')  # or yolov5m, yolov5l, yolov5x, custom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "adabe758-3d1f-4ad6-88d9-0abf470ed1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# yolov5 model settings:\n",
    "model.classes = [0] # only detect humans -> class = 0\n",
    "model.max_det = 3\n",
    "model.conf = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65dd195c-3168-4315-aeba-f032f584fdbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pose Classification Model\n",
    "interpreter = tf.lite.Interpreter(model_path='data/lite-model_movenet_singlepose_lightning_3.tflite')\n",
    "interpreter.allocate_tensors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f799d0b1-33b9-435c-a218-e850d38a9e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bbox(results, frame): #pd is a \n",
    "    pd = results.pandas().xyxy[0].sort_values('confidence')\n",
    "    \n",
    "    nb_detected_objs = len(pd.index)\n",
    "    \n",
    "    bbox = np.zeros((4,nb_detected_objs),dtype=int)#4 rows for x0,y0,x1,y1 --> each column is the bb of 1 img\n",
    "    confidence = np.zeros(nb_detected_objs)\n",
    "    class_names = np.zeros(nb_detected_objs,dtype=object)\n",
    "    images = []\n",
    "    \n",
    "    counter = 0\n",
    "    for obj in pd.iloc:\n",
    "        x0, y0, x1, y1 = obj.to_numpy()[0:4].astype(int)\n",
    "        images.append(frame[y0:y1, x0:x1])\n",
    "        confidence[counter] = obj.to_numpy()[4]\n",
    "        class_names[counter] = obj.to_numpy()[6]\n",
    "        #print(obj.to_numpy()[6])\n",
    "        bbox[:,counter] = np.array([x0,y0,x1,y1],dtype=int)\n",
    "        counter += 1\n",
    "    \n",
    "    #print(class_names)\n",
    "    return bbox, confidence, class_names, images\n",
    "\n",
    "def format_boxes(bboxes):#for deep sort\n",
    "    for box in bboxes:\n",
    "        ymin = int(box[1])\n",
    "        xmin = int(box[0])\n",
    "        ymax = int(box[3])\n",
    "        xmax = int(box[2])\n",
    "        width = xmax - xmin\n",
    "        height = ymax - ymin\n",
    "        box[0], box[1], box[2], box[3] = xmin, ymin, width, height\n",
    "    return bboxes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358cf5c6-3e96-4d6f-bb11-6b5c14129d54",
   "metadata": {},
   "source": [
    "# Functions used for Pose Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0572af9f-b3d2-4c90-913a-58a8a50c5068",
   "metadata": {},
   "source": [
    "## MoveNet output:\n",
    "\n",
    "A float32 tensor of shape [1, 1, 17, 3].\n",
    "\n",
    "● The first two channels of the last dimension represents the yx coordinates (normalized to\n",
    "image frame, i.e. range in [0.0, 1.0]) of the 17 keypoints (in the order of: [nose, left eye,\n",
    "right eye, left ear, right ear, left shoulder, right shoulder, left elbow, right elbow, left wrist,\n",
    "right wrist, left hip, right hip, left knee, right knee, left ankle, right ankle]).\n",
    "\n",
    "● The third channel of the last dimension represents the prediction confidence scores of\n",
    "each keypoint, also in the range [0.0, 1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10b2d8e2-8a99-4e1a-a499-70f628ead28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "EDGES = {\n",
    "    (0, 1): 'm',\n",
    "    (0, 2): 'c',\n",
    "    (1, 3): 'm',\n",
    "    (2, 4): 'c',\n",
    "    (0, 5): 'm',\n",
    "    (0, 6): 'c',\n",
    "    (5, 7): 'm',\n",
    "    (7, 9): 'm',\n",
    "    (6, 8): 'c',\n",
    "    (8, 10): 'c',\n",
    "    (5, 6): 'y',\n",
    "    (5, 11): 'm',\n",
    "    (6, 12): 'c',\n",
    "    (11, 12): 'y',\n",
    "    (11, 13): 'm',\n",
    "    (13, 15): 'm',\n",
    "    (12, 14): 'c',\n",
    "    (14, 16): 'c'\n",
    "}\n",
    "\n",
    "def calculateAngle(landmark1, landmark2, landmark3):\n",
    "    '''\n",
    "    This function calculates angle between three different landmarks.\n",
    "    Args:\n",
    "        landmark1: The first landmark containing the x,y and z coordinates.\n",
    "        landmark2: The second landmark containing the x,y and z coordinates.\n",
    "        landmark3: The third landmark containing the x,y and z coordinates.\n",
    "    Returns:\n",
    "        angle: The calculated angle between the three landmarks.\n",
    "    '''\n",
    "\n",
    "    # Get the required landmarks coordinates.\n",
    "    x1, y1 = landmark1\n",
    "    x2, y2 = landmark2\n",
    "    x3, y3 = landmark3\n",
    "\n",
    "    # Calculate the angle between the three points\n",
    "    angle = math.degrees(math.atan2(y3 - y2, x3 - x2) - math.atan2(y1 - y2, x1 - x2))\n",
    "    angle = np.abs(angle)\n",
    "    # Check if the angle is less than zero.\n",
    "    if angle > 180.0:\n",
    "\n",
    "        angle = 360-angle\n",
    "        \n",
    "    # Return the calculated angle.\n",
    "    return angle\n",
    "\n",
    "def draw_keypoints(frame, keypoints, confidence_threshold):\n",
    "    y, x, c = frame.shape\n",
    "    shaped = np.squeeze(np.multiply(keypoints, [y,x,1]))\n",
    "    \n",
    "    for kp in shaped:\n",
    "        ky, kx, kp_conf = kp\n",
    "        if kp_conf > confidence_threshold:\n",
    "            cv2.circle(frame, (int(kx), int(ky)), 4, (0,255,0), -1) \n",
    "            \n",
    "def draw_connections(frame, keypoints, edges, confidence_threshold):\n",
    "    y, x, c = frame.shape\n",
    "    shaped = np.squeeze(np.multiply(keypoints, [y,x,1]))\n",
    "    \n",
    "    for edge, color in edges.items():\n",
    "        p1, p2 = edge\n",
    "        y1, x1, c1 = shaped[p1]\n",
    "        y2, x2, c2 = shaped[p2]\n",
    "        \n",
    "        if (c1 > confidence_threshold) & (c2 > confidence_threshold):      \n",
    "            cv2.line(frame, (int(x1), int(y1)), (int(x2), int(y2)), (0,0,255), 2)\n",
    "            \n",
    "def classifyPose(kp_array, output_image, display=False):\n",
    "    '''\n",
    "    This function classifies yoga poses depending upon the angles of various body joints.\n",
    "    Args:\n",
    "        kp_array: A list of detected landmarks of the person whose pose needs to be classified.\n",
    "        output_image: A image of the person with the detected pose landmarks drawn.\n",
    "        display: A boolean value that is if set to true the function displays the resultant image with the pose label \n",
    "        written on it and returns nothing.\n",
    "    Returns:\n",
    "        output_image: The image with the detected pose landmarks drawn and pose label written.\n",
    "        label: The classified pose label of the person in the output_image.\n",
    "\n",
    "    '''\n",
    "    \n",
    "    # Initialize the label of the pose. It is not known at this stage.\n",
    "    label = 'Unknown Pose'\n",
    "\n",
    "    # Specify the color (Red) with which the label will be written on the image.\n",
    "    color = (0, 0, 255)\n",
    "    \n",
    "    # Calculate the required angles.\n",
    "    #----------------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    # Get the angle between the left shoulder, elbow and wrist points. \n",
    "    left_elbow_angle = calculateAngle(kp_array[5],\n",
    "                                      kp_array[7],\n",
    "                                      kp_array[9])\n",
    "    \n",
    "    # Get the angle between the right shoulder, elbow and wrist points.\n",
    "    right_elbow_angle = calculateAngle(kp_array[6],\n",
    "                                       kp_array[8],\n",
    "                                       kp_array[10])\n",
    "    \n",
    "    # Get the angle between the left elbow, shoulder and hip points.\n",
    "    left_shoulder_angle = calculateAngle(kp_array[7],\n",
    "                                         kp_array[5],\n",
    "                                         kp_array[11])\n",
    "    # Get the angle between the right hip, shoulder and elbow points.\n",
    "    right_shoulder_angle = calculateAngle(kp_array[12],\n",
    "                                          kp_array[6],\n",
    "                                          kp_array[8])\n",
    "    \n",
    "    #print('left_elbow_angle: ', left_elbow_angle ,'\\n right_elbow_angle: ', right_elbow_angle)\n",
    "    #print('left_shoulder_angle: ', left_shoulder_angle ,'\\n right_shoulder_angle: ', right_shoulder_angle)\n",
    "    \n",
    "    #----------------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    # Check if it is the warrior II pose or the T pose.\n",
    "    # As for both of them, both arms should be straight and shoulders should be at the specific angle.\n",
    "    #----------------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    # Check if the both arms are straight.\n",
    "    if left_elbow_angle > 125 and left_elbow_angle < 220 and right_elbow_angle > 125 and right_elbow_angle < 220:\n",
    "        #label = 'T Pose'\n",
    "        # Check if shoulders are at the required angle.\n",
    "        if left_shoulder_angle > 70 and left_shoulder_angle < 110 and right_shoulder_angle > 70 and right_shoulder_angle < 110:\n",
    "            label = 'T Pose'\n",
    "                        \n",
    "    if right_elbow_angle > 50 and right_elbow_angle < 130 and right_shoulder_angle > 70 and right_shoulder_angle < 110:\n",
    "        label = \"power to the people\"\n",
    "    # Check if the pose is classified successfully\n",
    "    if label != 'Unknown Pose':\n",
    "        \n",
    "        # Update the color (to green) with which the label will be written on the image.\n",
    "        color = (0, 255, 0)  \n",
    "    \n",
    "    # Write the label on the output image. \n",
    "    cv2.putText(output_image, label, (10, 30),cv2.FONT_HERSHEY_PLAIN, 1, color, 2)\n",
    "    \n",
    "    # Check if the resultant image is specified to be displayed.\n",
    "    if display:\n",
    "    \n",
    "        # Display the resultant image.\n",
    "        plt.figure(figsize=[10,10])\n",
    "        plt.imshow(output_image[:,:,::-1]);plt.title(\"Output Image\");plt.axis('off');\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        # Return the output image and the classified label.\n",
    "        return output_image, label\n",
    "    \n",
    "def get_pose_from_image_and_bounding_box(bbox, frame, bbox_width, bbox_height):\n",
    "    #IDEA: THE POSE HAS TO BE DETECTED IN 10 CONSECUTIVE IMAGES\n",
    "    \n",
    "    \n",
    "    bbox = np.int32(bbox.clip(min=0))\n",
    "    \n",
    "    WIDTH_THRESHOLD,HEIGHT_THRESHOLD = 100,100\n",
    "    \n",
    "    #add condition to neglect bounding boxes under a certain size threshold\n",
    "    bbox_size_sufficient = False\n",
    "    \n",
    "    if bbox_width > WIDTH_THRESHOLD and bbox_height > HEIGHT_THRESHOLD:\n",
    "        bbox_size_sufficient = True\n",
    "        \n",
    "    \n",
    "    if len(bbox)>0 and np.all(bbox>0) and bbox_size_sufficient:#we do not want to cover the cases where we are the boundary of the image --> pose classification behaves strangely\n",
    "        \n",
    "        img = frame[bbox[1]:bbox[3], bbox[0]:bbox[2]]\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "\n",
    "        #perform movenet on img(img is the cropped image)\n",
    "        img1 = img.copy()\n",
    "        img1 = tf.image.resize_with_pad(np.expand_dims(img1, axis=0), 192,192)\n",
    "        input_image = tf.cast(img1, dtype=tf.float32)\n",
    "\n",
    "        # Setup input and output \n",
    "        input_details = interpreter.get_input_details()\n",
    "        output_details = interpreter.get_output_details()\n",
    "\n",
    "        # Make predictions \n",
    "        interpreter.set_tensor(input_details[0]['index'], np.array(input_image))\n",
    "        interpreter.invoke()\n",
    "        keypoints_with_scores = interpreter.get_tensor(output_details[0]['index'])\n",
    "\n",
    "        # Rendering \n",
    "        draw_connections(img, keypoints_with_scores, EDGES, 0.4)\n",
    "        draw_keypoints(img, keypoints_with_scores, 0.4)\n",
    "        \n",
    "        #cv2.putText(img, \"left hip condfidence\" + str(keypoints_with_scores.reshape((17,3))[11,2]),(int(bbox[0]), int(bbox[1]-10)),0, 0.75, (255,255,255),2)  \n",
    "        \n",
    "        landmarks = keypoints_with_scores.reshape((17,3))[:,0:2]#array of landmarks (x,y)\n",
    "\n",
    "        label = \"\"\n",
    "        #Classification\n",
    "        if np.size(landmarks) != 0:\n",
    "            img, label = classifyPose(landmarks, img, display=False)\n",
    "            \n",
    "        \n",
    "        cv2.imshow('Pose Landmarks',img)\n",
    "    else:\n",
    "        img = frame\n",
    "        label = \"no bounding box detected\"\n",
    "    return img, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326bb48e-b81f-4594-8743-f0e604bb155f",
   "metadata": {},
   "source": [
    "# Main Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c335242e-a44d-4f0a-8975-70003a8aca98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WE ARE IN THE SECOND LOOP\n",
      "WE ARE IN THE SECOND LOOP\n",
      "WE ARE IN THE SECOND LOOP\n",
      "WE ARE IN THE SECOND LOOP\n",
      "WE ARE IN THE SECOND LOOP\n",
      "WE ARE IN THE SECOND LOOP\n",
      "WE ARE IN THE SECOND LOOP\n",
      "WE ARE IN THE SECOND LOOP\n",
      "WE ARE IN THE SECOND LOOP\n",
      "WE ARE IN THE SECOND LOOP\n"
     ]
    }
   ],
   "source": [
    "#Definition of the parameters\n",
    "max_cosine_distance = 0.1 #The matching threshold. Samples with larger distance are considered an invalid match.\n",
    "nn_budget = None #[int] If not None, fix samples per class to at most this number. Removes the oldest samples when the budget is reached.\n",
    "nms_max_overlap = 1.0 # ROIs that overlap more than this values are suppressed.(non_max_suppression)\n",
    "\n",
    "#Parameters for the Tracker():\n",
    "max_age = 500#Maximum number of missed misses before a track is deleted.\n",
    "n_init = 10\n",
    "max_iou_distance = 0.7\n",
    "\n",
    "\n",
    "\n",
    "# initialize deep sort\n",
    "model_filename = 'model_data/mars-small128.pb'\n",
    "encoder = gdet.create_box_encoder(model_filename, batch_size=1)\n",
    "# calculate cosine distance metric\n",
    "metric = nn_matching.NearestNeighborDistanceMetric(\"cosine\", max_cosine_distance, nn_budget)\n",
    "# initialize tracker\n",
    "tracker = Tracker(metric,max_iou_distance,max_age, n_init)\n",
    "\n",
    "try:\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    #cap = cv2.VideoCapture(\"C:\\DLAV_testing\\images to test\\Business-people-dancing1.mp4\")\n",
    "except:\n",
    "    print(\"webcam/video could not be loaded\")\n",
    "\n",
    "initialized = False\n",
    "frame_num = 0\n",
    "first_pass = True\n",
    "pose_really_detected = False\n",
    "\n",
    "pose_list = np.zeros(20,dtype=object)\n",
    "\n",
    "trigger_id = None\n",
    "\n",
    "while True:\n",
    "    return_value, frame = cap.read()\n",
    "    if return_value:\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        image = Image.fromarray(frame)\n",
    "    else:\n",
    "        print('Video has ended or failed, try a different video format!')\n",
    "        break\n",
    "    frame_num +=1\n",
    "    \n",
    "    #print('Frame #: ', frame_num)\n",
    "    start_time = time.time()\n",
    "    \n",
    "    #yolo comes in place\n",
    "    results = model(frame)#inference from yolo\n",
    "    \n",
    "    frame = results.render()[0]\n",
    "    \n",
    "    boxes, scores, names, imglist = get_bbox(results, frame)\n",
    "    \n",
    "    #frame, label = write_frame_keypoints(imglist,bboxes,frame)#movenet\n",
    "    \n",
    "    bboxes = format_boxes(boxes.transpose())# certain format of bboxes for the tracker\n",
    "    \n",
    "    features = encoder(frame, bboxes)\n",
    "    detections = [Detection(bbox, score, class_name, feature) for bbox, score, class_name, feature in zip(bboxes, scores, names, features)]\n",
    "    \n",
    "    #initizialize color map\n",
    "    cmap = plt.get_cmap('tab20b')    \n",
    "    \n",
    "    # run non-maxima supression\n",
    "    boxs = np.array([d.tlwh for d in detections])\n",
    "    scores = np.array([d.confidence for d in detections])\n",
    "    classes = np.array([d.class_name for d in detections])\n",
    "    indices = preprocessing.non_max_suppression(boxs, classes, nms_max_overlap, scores)\n",
    "    detections = [detections[i] for i in indices]             \n",
    "        \n",
    "    #call the tracker\n",
    "    tracker.predict()\n",
    "    tracker.update(detections)\n",
    "    #tracker.tracks = [t for t in tracker.tracks if (t.track_id ==1)] # this doesnt really work --> the features of other people are not taken into account\n",
    "    \n",
    "    \n",
    "    \n",
    "    colors = [cmap(i)[:3] for i in np.linspace(0, 1, 20)]\n",
    "    # update tracks\n",
    "    \n",
    "    for track in tracker.tracks:\n",
    "        if not track.is_confirmed() or track.time_since_update > 1:\n",
    "            continue \n",
    "            \n",
    "        bbox = track.to_tlbr()#sometimes the bounding boxes were negative\n",
    "        _, __, bbox_width, bbox_height = track.to_tlwh() #used to get the width and the height of the bbox --> compared against threshold --> neglect small bounding boxes\n",
    "        class_name = track.get_class()       \n",
    "        \n",
    "        img, label = get_pose_from_image_and_bounding_box(bbox, frame, bbox_width, bbox_height)\n",
    "        \n",
    "        \n",
    "        \n",
    "        if initialized == False:\n",
    "        #pose estimation:\n",
    "        \n",
    "            #img, label = get_pose_from_image_and_bounding_box(bboxes.flatten(),frame)\n",
    "            pose_list = np.append(pose_list,label)\n",
    "            pose_list = np.delete(pose_list,0) #delete the first item of the list --> keep the list short\n",
    "        \n",
    "            if(np.all(pose_list == \"power to the people\")):\n",
    "                pose_really_detected = True\n",
    "            \n",
    "        \n",
    "            #print(label, \"   first pass: \" ,first_pass, \"      initialized: \", initialized)\n",
    "        \n",
    "            #saving the POI ID\n",
    "            if label == 'power to the people' and first_pass == True and pose_really_detected == True:\n",
    "                trigger_id = track.track_id            \n",
    "                tracker.tracks = [t for t in tracker.tracks if (t.track_id==trigger_id)]\n",
    "                first_pass = False \n",
    "                initialized = True\n",
    "                #cv2.destroyWindow('Pose Landmarks')\n",
    "        \n",
    "        if track.track_id == trigger_id:\n",
    "            #if track.track_id == trigger_track.track_id:\n",
    "            # draw bbox on screen\n",
    "            #print(\"WE ARE IN THE SECOND LOOP\")\n",
    "            color = colors[int(track.track_id) % len(colors)]\n",
    "            color = [i * 255 for i in color]\n",
    "            cv2.rectangle(frame, (int(bbox[0]), int(bbox[1])), (int(bbox[2]), int(bbox[3])), color, 2)\n",
    "            cv2.rectangle(frame, (int(bbox[0]), int(bbox[1]-30)), (int(bbox[0])+(len(class_name)+len(str(track.track_id)))*17, int(bbox[1])), color, -1)\n",
    "            cv2.putText(frame, class_name + \"-\" + str(track.track_id),(int(bbox[0]), int(bbox[1]-10)),0, 0.75, (255,255,255),2)\n",
    "            #cv2.putText(frame, \"My human\",(int(bbox[0]), int(bbox[1]-10)),0, 0.75, (255,255,255),2)        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        #color = colors[int(track.track_id) % len(colors)]\n",
    "        #color = [i * 255 for i in color]\n",
    "        #cv2.rectangle(frame, (int(bbox[0]), int(bbox[1])), (int(bbox[2]), int(bbox[3])), color, 2)\n",
    "        #cv2.rectangle(frame, (int(bbox[0]), int(bbox[1]-30)), (int(bbox[0])+(len(class_name)+len(str(track.track_id)))*17, int(bbox[1])), color, -1)\n",
    "        #cv2.putText(frame, class_name + \"-\" + str(track.track_id),(int(bbox[0]), int(bbox[1]-10)),0, 0.75, (255,255,255),2)        \n",
    "        \n",
    "    # calculate frames per second of running detections\n",
    "    fps = 1.0 / (time.time() - start_time)\n",
    "    #print(\"FPS: %.2f\" % fps)\n",
    "    result = np.asarray(frame)\n",
    "    result = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR) \n",
    "    \n",
    "    \n",
    "    \n",
    "    cv2.imshow('YoloV5 + Deep Sort', result)\n",
    "    if cv2.waitKey(10) & 0xFF==ord('q'):\n",
    "        break\n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f335450-03cf-48f6-bea2-ae250402e851",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.destroyAllWindows()\n",
    "cap.release()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bac35b1da89963a84e831f1e7ac9d5013744a77e967380d6e4824954f258b8e9"
  },
  "kernelspec": {
   "display_name": "Python 3.7.0 ('yolov4-cpu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
